# NLP Assignment 1: Multilingual Word Embeddings

**Author:** ADAM Muhammad Safi Ullah | p2516297

Exploration and alignment of word embeddings for English and French, including linguistic analysis and a downstream language identification task.

---

## Project Overview

This project builds and aligns English and French word embeddings into a shared semantic space. The aligned embeddings are analyzed linguistically and evaluated through a downstream classification task.

---

## Repository Structure

project/
â”œâ”€â”€ 01_workflow.ipynb # Main Colab notebook (run all cells)
â”œâ”€â”€ results_summary.json # Alignment, analysis & classifier metrics
â”œâ”€â”€ en-fr.txt # MUSE bilingual dictionary used for alignment
â”œâ”€â”€ NLP_Assignment1_Report.pdf # Final report (main deliverable)
â””â”€â”€ README.md # Project documentation

---

## â–¶ï¸ How to Run (Google Colab)

1. Open the notebook in Colab.
2. Run all cells sequentially.

The notebook will:

1. Load and preprocess the ENâ€“FR parallel corpus  
2. Train word embeddings (Word2Vec & FastText)  
3. Load MUSE bilingual dictionary  
4. Align embeddings using **Procrustes mapping**  
5. Analyze semantic properties  
6. Visualise embedding spaces  
7. Train a language identification classifier  

---

## ğŸ”¬ Methods Used

### ğŸ”¹ Embeddings
- Word2Vec  
- FastText (subword-aware; used for alignment)

### ğŸ”¹ Multilingual Alignment
- MUSE bilingual dictionary (~113k pairs)
- Orthogonal Procrustes mapping
- English vectors mapped into French space

### ğŸ”¹ Linguistic Analysis
- Synonyms vs antonyms similarity
- Translation similarity (common words)
- Polysemy example (*bank*)
- OOV coverage evaluation

### ğŸ”¹ Visualization
- PCA projection of embeddings
- t-SNE semantic clustering
- Multilingual PCA showing alignment

### ğŸ”¹ Downstream Task
Language identification using aligned sentence embeddings.

---

## ğŸ“Š Results

### Multilingual Alignment
- Alignment achieved using Procrustes mapping
- Cosine similarities indicate meaningful shared space

### Linguistic Insights
- Synonyms show moderate similarity
- Antonyms may appear similar (distributional semantics effect)
- FastText provides strong OOV handling

### Downstream Classification
Using aligned embeddings:

- **Accuracy:** â‰ˆ 97%  
- **Precision:** â‰ˆ 0.97  
- **Recall:** â‰ˆ 0.97  
- **F1-score:** â‰ˆ 0.97  

This confirms aligned embeddings retain meaningful linguistic structure.

---

## ğŸ“‚ Output File

### `results_summary.json`
Contains:

- alignment metrics  
- synonym vs antonym similarity  
- common word similarity  
- polysemy analysis  
- OOV coverage  
- classifier performance  

---

## ğŸ§  Key Takeaways

âœ” FastText improves vocabulary coverage and robustness  
âœ” Procrustes alignment enables cross-lingual semantic mapping  
âœ” Distributional embeddings capture similarity but struggle with antonyms  
âœ” Visualization confirms successful alignment  
âœ” High classification accuracy validates embedding usefulness  

---

## ğŸ›  Libraries Used

- gensim  
- scikit-learn  
- numpy  
- matplotlib  
- scipy  

---

## ğŸ“ Academic Context

UniversitÃ© Claude Bernard Lyon 1  
Master: Data & Intelligence for Smart Systems (DISS)

---

## â­ Open in Google Colab

*(optional â€” add your Colab link here)*

